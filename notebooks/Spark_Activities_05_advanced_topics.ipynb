{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "1. Wikipedia page on Python programming language - **./data/python_wiki.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"../../spark2/\")\n",
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"helloworld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1246"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test our setup by counting the number of nonempty lines in a text file\n",
    "lines = sc.textFile('../data/python_wiki.html')\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "lines_nonempty.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5929"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test our setup by counting the number of nonempty lines in a text file\n",
    "lines = sc.textFile('../data/enron.json')\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "lines_nonempty.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD = sc.parallelize([1,2,3,4], 2)\n",
    "numRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(numRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[13] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD.map(lambda x : x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9436df7ec9c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minside\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pi is roughly %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "n = 1000000\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, n)) \\\n",
    "             .filter(inside).count()\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PairRDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some Spark operations are available only on RDDs of key-value pairs\n",
    "* Most of these operations, except counting operations, usually end up with shuffled data (because all the data for a given key may not be inside the same partition)\n",
    "\n",
    "Let's look at an example: The Cartesian Operation\n",
    "\n",
    "The Cartesian operation, applied on a given dataset, takes another dataset as parameter and returns the cartesian product of both the daasets as the output.\n",
    "\n",
    "If x has m elements and y has n elements, how many elements does their cartesian product have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10),\n",
       " (1, 11),\n",
       " (1, 12),\n",
       " (2, 10),\n",
       " (2, 11),\n",
       " (2, 12),\n",
       " (3, 10),\n",
       " (3, 11),\n",
       " (3, 12)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3])\n",
    "y = sc.parallelize([10,11,12])\n",
    "\n",
    "x.cartesian(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The groupByKey and reduceByKey actions\n",
    "\n",
    "Let's investigate the additional transformations:\n",
    "* [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) and\n",
    "* [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey).\n",
    "\n",
    "Both of these transformations operate on pair RDDs.\n",
    "\n",
    "* A pair RDD is an RDD where each element is a pair tuple (key, value).\n",
    "* For example, `sc.parallelize([('a', 1), ('a', 2), ('b', 1)])` would create a pair RDD where the keys are 'a', 'a', 'b' and the values are 1, 2, 1.\n",
    "\n",
    "\n",
    "* `reduceByKey()` gathers together pairs that have the same key and applies a function to two associated values at a time.\n",
    "* `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions.\n",
    "\n",
    "\n",
    "* While both the `groupByKey()` and `reduceByKey()` transformations can often be used to solve the same problem and will produce the same answer, the `reduceByKey()` transformation works much better for large distributed datasets.\n",
    "* This is because Spark knows it can combine output with a common key on each partition *before* shuffling (redistributing) the data across nodes.\n",
    "* Only use `groupByKey()` if the operation would not benefit from reducing the data before the shuffle occurs.\n",
    "\n",
    "\n",
    "Look at the diagram below to understand how `reduceByKey` works.\n",
    "\n",
    "* Notice how pairs on the same machine with the same key are combined (by using the lamdba function passed into reduceByKey) before the data is shuffled.\n",
    "* Then the lamdba function is called again to reduce all the values from each partition to produce one final result.\n",
    "\n",
    "![reduceByKey() figure](http://spark-mooc.github.io/web-assets/images/reduce_by.png)\n",
    "\n",
    "* On the other hand, when using the `groupByKey()` transformation - all the key-value pairs are shuffled around, causing a lot of unnecessary data to being transferred over the network.\n",
    "* To determine which machine to shuffle a pair to, Spark calls a partitioning function on the key of the pair.\n",
    "\n",
    "\n",
    "* Spark spills data to disk when there is more data shuffled onto a single executor machine than can fit in memory.\n",
    "* However, it flushes out the data to disk one key at a time, so if a single key has more key-value pairs than can fit in memory an out of memory exception occurs.\n",
    "* This is more gracefully handled in later releases of Spark so that the job can still proceed, but should still be avoided.\n",
    "* When Spark needs to spill to disk, performance is severely impacted.\n",
    "\n",
    "![groupByKey() figure](http://spark-mooc.github.io/web-assets/images/group_by.png)\n",
    "\n",
    "As your dataset grows, the difference in the amount of data that needs to be shuffled, between the `reduceByKey()` and `groupByKey()` transformations, becomes increasingly exaggerated.\n",
    "\n",
    "\n",
    "Here are more transformations to prefer over `groupByKey()`:\n",
    "* [combineByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.combineByKey) can be used when you are combining elements but your return type differs from your input value type.\n",
    "* [foldByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.foldByKey) merges the values for each key using an associative function and a neutral \"zero value\".\n",
    "\n",
    "Now let's go through a simple `groupByKey()` and `reduceByKey()` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', [1]), ('a', [1, 2])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD = sc.parallelize([('a', 1), ('a', 2), ('b', 1)])\n",
    "\n",
    "# mapValues only used to improve format for printing\n",
    "pairRDD.groupByKey().mapValues(lambda x: list(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('a', 3)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different ways to sum by key\n",
    "pairRDD.groupByKey().map(lambda k: (k[0], sum(k[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 1), ('a', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Using mapValues, which is recommended when they key doesn't change\n",
    "pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 1), ('a', 3)]\n"
     ]
    }
   ],
   "source": [
    "def add(x, y):\n",
    "    return(x + y)\n",
    "\n",
    "# reduceByKey is more efficient / scalable\n",
    "print(pairRDD.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More groupByKey examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black', 'blue', 'white', 'green', 'grey']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([\"black\", \"blue\", \"white\", \"green\", \"grey\"], 2)\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, <pyspark.resultiterable.ResultIterable at 0x117a1a390>),\n",
       " (5, <pyspark.resultiterable.ResultIterable at 0x117a1a320>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.groupBy(lambda x: len(x)).collect()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ['blue', 'grey']), (5, ['black', 'green', 'white'])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x,sorted(y)) for (x,y) in b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More reduceByKey examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 'bluegrey'), (5, 'blackwhitegreen')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([\"black\", \"blue\", \"white\", \"green\", \"grey\"], 2)\n",
    "b = a.map(lambda x: (len(x), x))\n",
    "b.reduceByKey(lambda x,y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 'blue'), (6, 'orange'), (5, 'blackwhite')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([\"black\", \"blue\", \"white\", \"orange\"], 2)\n",
    "b = a.map(lambda x: (len(x), x))\n",
    "b.reduceByKey(lambda x,y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The join operation\n",
    "\n",
    "* Both datasets should be of key-value pair type (pairRDDs)\n",
    "* The resulting datasets will have all the key-value pairs from both the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = sc.parallelize([\"blue\", \"green\", \"orange\"], 3)\n",
    "b = a.keyBy(lambda x: len(x))\n",
    "c = sc.parallelize([\"black\", \"white\", \"grey\"], 3)\n",
    "d = c.keyBy(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'green', 'orange']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 'blue'), (5, 'green'), (6, 'orange')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black', 'white', 'grey']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 'black'), (5, 'white'), (4, 'grey')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ('blue', 'grey')), (5, ('green', 'black')), (5, ('green', 'white'))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.join(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, ('orange', None)),\n",
       " (4, ('blue', 'grey')),\n",
       " (5, ('green', 'black')),\n",
       " (5, ('green', 'white'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#leftOuterJoin\n",
    "b.leftOuterJoin(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ('blue', 'grey')), (5, ('green', 'black')), (5, ('green', 'white'))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rightOuterJoin\n",
    "b.rightOuterJoin(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, ('orange', None)),\n",
       " (4, ('blue', 'grey')),\n",
       " (5, ('green', 'black')),\n",
       " (5, ('green', 'white'))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fullOuterJoin\n",
    "b.fullOuterJoin(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
