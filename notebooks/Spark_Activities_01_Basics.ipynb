{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1: Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c4e08c1c1130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# provide path to your spark directory directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bdap/lib/python3.5/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# add pyspark to sys.path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mspark_python\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'py4j-*.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "# provide path to your spark directory directly\n",
    "findspark.init(\"../../spark2\")\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, so we have an error. Now what?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did you start the Spark instance first?**\n",
    "\n",
    "        cd spark2/sbin\n",
    "        ./start-master.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have you specifed the path correctly?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f95c089306fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# provide path to your spark directory directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../spark2/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bdap/lib/python3.5/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# add pyspark to sys.path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mspark_python\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'py4j-*.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "# provide path to your spark directory directly\n",
    "findspark.init(\"../../spark2/\")\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's create a SparkContext and use it to count the number of lines in a file. For that, let's create a text file first.**\n",
    "\n",
    "        cd\n",
    "        ls >> helloworld\n",
    "        cat helloworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\\nworld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"helloworld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nonempty(x):\n",
    "    return len(x) > 0\n",
    "    \n",
    "# let's test our setup by counting the number of nonempty lines in a text file\n",
    "lines = sc.textFile('../README.md')\n",
    "lines_nonempty = lines.filter(nonempty)\n",
    "lines_nonempty.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "lines_nonempty.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "../README.md MapPartitionsRDD[6] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_nonempty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Learn Spark2 with Python',\n",
       " '',\n",
       " '1. [Set up your machine](https://github.com/soumendra/learn-spark-python/blob/master/setting-up.md)',\n",
       " '2. [Go through the pre-class reading list](https://github.com/soumendra/learn-spark-python/blob/master/pre-course-reading.md)',\n",
       " '']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "lines_nonempty.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, so we can't run multiple SparkContexts at once! What about running the one created before?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's test our setup by counting the number of nonempty lines in a text file\n",
    "lines = sc.textFile('README.md')\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "lines_nonempty.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2: Using Anonyous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's use *lambda* to create an anonymous function to count the number of lines containing *Python*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a fun language,\n",
      " but then what language\n",
      " is not, if\n",
      " I may ask. But Python\n",
      " is also.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "text=\"Python is a fun language,\\n\n",
    "but then what language\\n\n",
    "is not, if\\n\n",
    "I may ask. But Python\\n\n",
    "is also.\"\n",
    "\n",
    "echo -e $text > python.txt\n",
    "cat python.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of lines containing 'Python': 2\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"python.txt\")\n",
    "pythonLines = lines.filter(lambda line: \"Python\" in line)\n",
    "print(\"No of lines containing 'Python':\", pythonLines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well, do explain the answer.**\n",
    "\n",
    "* Task: Count the no of lines in the wikipedia page for Python which have the word Python in them.\n",
    "* Hint\n",
    "    - Use the [requests](http://docs.python-requests.org/en/master/) package to read a URL\n",
    "    - Use the [lxml]() package to extract the text content out of the html\n",
    "    - [Understand how to write a string to a file](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/soumendra/anaconda3/envs/bdap/lib/python3.5/site-packages\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/soumendra/anaconda3/envs/bdap/lib/python3.5/site-packages (from requests)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/soumendra/anaconda3/envs/bdap/lib/python3.5/site-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/soumendra/anaconda3/envs/bdap/lib/python3.5/site-packages (from requests)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/soumendra/anaconda3/envs/bdap/lib/python3.5/site-packages (from requests)\n",
      "Requirement already satisfied: lxml in /Users/soumendra/anaconda3/envs/bdap/lib/python3.5/site-packages\n",
      "Collecting html2text\n",
      "  Downloading html2text-2016.9.19.tar.gz (47kB)\n",
      "Building wheels for collected packages: html2text\n",
      "  Running setup.py bdist_wheel for html2text: started\n",
      "  Running setup.py bdist_wheel for html2text: finished with status 'done'\n",
      "  Stored in directory: /Users/soumendra/Library/Caches/pip/wheels/96/13/e0/25f9de1c524662d264bb143dde112812b72789bc8058dc4f57\n",
      "Successfully built html2text\n",
      "Installing collected packages: html2text\n",
      "Successfully installed html2text-2016.9.19\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate bdap\n",
    "pip install requests\n",
    "pip install lxml\n",
    "pip install html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from html2text import html2text\n",
    "\n",
    "r = requests.get('https://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "if r.status_code==200:\n",
    "    with open('python_html', 'w') as f:\n",
    "        f.write(html2text(r.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[![This is a good article. Click here for more\n",
      "information.](//upload.wikimedia.org/wikipedia/en/thumb/9/94/Symbol_support_vote.svg/19px-\n",
      "Symbol_support_vote.svg.png)](/wiki/Wikipedia:Good_articles \"This is a good\n",
      "article. Click here for more information.\" )\n",
      "\n",
      "# Python (programming language)\n",
      "\n",
      "From Wikipedia, the free encyclopedia\n",
      "\n",
      "Jump to: navigation, search\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head python_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of lines containing 'Python': 375\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"python_html\")\n",
    "pythonLines = lines.filter(lambda line: \"Python\" in line)\n",
    "print(\"No of lines containing 'Python':\", pythonLines.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 3: Counting Primes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We’ll go ahead and calculate the number of primes less than a given large number. To start with, we'll define a function that determines the primality of any given number (we'll later parallelize this function on a set of numbers).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isprime(n):\n",
    "    \"\"\"\n",
    "    check if integer n is a prime\n",
    "    \"\"\"\n",
    "    # make sure n is a positive integer\n",
    "    n = abs(int(n))\n",
    "    # 0 and 1 are not primes\n",
    "    if n < 2:\n",
    "        return False\n",
    "    # 2 is the only even prime number\n",
    "    if n == 2:\n",
    "        return True\n",
    "    # all other even numbers are not primes\n",
    "    if not n & 1:\n",
    "        return False\n",
    "    # range starts with 3 and only needs to go up the square root of n\n",
    "    # for all odd numbers\n",
    "    for x in range(3, int(n**0.5)+1, 2):\n",
    "        if n % x == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78498\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD of numbers from 0 to 1,000,000\n",
    "nums = sc.parallelize(range(1000000))\n",
    "\n",
    "# Compute the number of primes in the RDD\n",
    "print(nums.filter(isprime).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 4:  Word and Line Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from operator import add\n",
    "\n",
    "filein = sc.textFile('../README.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count lines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines in file: 42\n"
     ]
    }
   ],
   "source": [
    "print('number of lines in file: %s' % filein.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "../README.md MapPartitionsRDD[31] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Learn Spark2 with Python',\n",
       " '',\n",
       " '1. [Set up your machine](https://github.com/soumendra/learn-spark-python/blob/master/setting-up.md)',\n",
       " '2. [Go through the pre-class reading list](https://github.com/soumendra/learn-spark-python/blob/master/pre-course-reading.md)',\n",
       " '',\n",
       " '# Setting up AWS instance',\n",
       " '',\n",
       " 'When you are logging into a new ec2 instance for the first time, execute the following:',\n",
       " '',\n",
       " '```bash',\n",
       " 'sudo apt-get update -y',\n",
       " 'sudo apt-get upgrade -y',\n",
       " 'sudo apt-get install -y python-dev software-properties-common curl default-jre ',\n",
       " 'sudo apt-get install -y default-jdk python-software-properties byobu vim',\n",
       " '',\n",
       " 'sudo apt-get install git git-core',\n",
       " 'git config --global user.email \"you@example.com\"',\n",
       " 'git config --global user.name \"Your Name\"',\n",
       " '```',\n",
       " '* Set up anaconda - https://github.com/soumendra/python-machinelearning-setup',\n",
       " '* clone and install from spark.yml',\n",
       " '',\n",
       " '',\n",
       " '```bash',\n",
       " 'jupyter notebook --generate-config',\n",
       " 'mkdir certs',\n",
       " 'cd certs',\n",
       " 'cd ~/.jupyter',\n",
       " 'vim jupyter_notebook_config.py',\n",
       " '```',\n",
       " '',\n",
       " 'content:',\n",
       " '',\n",
       " '> c = get_config()',\n",
       " '',\n",
       " \"> c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem'\",\n",
       " '',\n",
       " \"> c.NotebookApp.ip = '*'\",\n",
       " '',\n",
       " '> c.NotebookApp.open_browser = False ',\n",
       " '',\n",
       " '> c.NotebookApp.port = 8888']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filein.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count non-empty lines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of non-empty lines in file: 29\n"
     ]
    }
   ],
   "source": [
    "filein_nonempty = filein.filter( lambda x: len(x) > 0 )\n",
    "print('number of non-empty lines in file: %s' % filein_nonempty.count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count no of characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters in file: 1080\n"
     ]
    }
   ],
   "source": [
    "chars = filein.map(lambda s: len(s)).reduce(add)\n",
    "print('number of characters in file: %s' % chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count words of length greater than 3 characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn',\n",
       " 'spark2',\n",
       " 'with',\n",
       " 'python',\n",
       " 'your',\n",
       " 'machine',\n",
       " 'https',\n",
       " 'github',\n",
       " 'soumendra',\n",
       " 'learn',\n",
       " 'spark',\n",
       " 'python',\n",
       " 'blob',\n",
       " 'master',\n",
       " 'setting',\n",
       " 'through',\n",
       " 'class',\n",
       " 'reading',\n",
       " 'list',\n",
       " 'https',\n",
       " 'github',\n",
       " 'soumendra',\n",
       " 'learn',\n",
       " 'spark',\n",
       " 'python',\n",
       " 'blob',\n",
       " 'master',\n",
       " 'course',\n",
       " 'reading',\n",
       " 'setting',\n",
       " 'instance',\n",
       " 'when',\n",
       " 'logging',\n",
       " 'into',\n",
       " 'instance',\n",
       " 'first',\n",
       " 'time',\n",
       " 'execute',\n",
       " 'following',\n",
       " 'bash',\n",
       " 'sudo',\n",
       " 'update',\n",
       " 'sudo',\n",
       " 'upgrade',\n",
       " 'sudo',\n",
       " 'install',\n",
       " 'python',\n",
       " 'software',\n",
       " 'properties',\n",
       " 'common',\n",
       " 'curl',\n",
       " 'default',\n",
       " 'sudo',\n",
       " 'install',\n",
       " 'default',\n",
       " 'python',\n",
       " 'software',\n",
       " 'properties',\n",
       " 'byobu',\n",
       " 'sudo',\n",
       " 'install',\n",
       " 'core',\n",
       " 'config',\n",
       " 'global',\n",
       " 'user',\n",
       " 'email',\n",
       " 'example',\n",
       " 'config',\n",
       " 'global',\n",
       " 'user',\n",
       " 'name',\n",
       " 'your',\n",
       " 'name',\n",
       " 'anaconda',\n",
       " 'https',\n",
       " 'github',\n",
       " 'soumendra',\n",
       " 'python',\n",
       " 'machinelearning',\n",
       " 'setup',\n",
       " 'clone',\n",
       " 'install',\n",
       " 'from',\n",
       " 'spark',\n",
       " 'bash',\n",
       " 'jupyter',\n",
       " 'notebook',\n",
       " 'generate',\n",
       " 'config',\n",
       " 'mkdir',\n",
       " 'certs',\n",
       " 'certs',\n",
       " 'jupyter',\n",
       " 'jupyter_notebook_config',\n",
       " 'content',\n",
       " 'get_config',\n",
       " 'notebookapp',\n",
       " 'certfile',\n",
       " 'home',\n",
       " 'ubuntu',\n",
       " 'certs',\n",
       " 'mycert',\n",
       " 'notebookapp',\n",
       " 'notebookapp',\n",
       " 'open_browser',\n",
       " 'false',\n",
       " 'notebookapp',\n",
       " 'port',\n",
       " '8888']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = filein.flatMap(lambda line: re.split('\\W+', line.lower().strip()))\n",
    "words = words.filter(lambda x: len(x) > 3)\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 'learn', 'spark2', 'with', 'python'],\n",
       " ['1',\n",
       "  'set',\n",
       "  'up',\n",
       "  'your',\n",
       "  'machine',\n",
       "  'https',\n",
       "  'github',\n",
       "  'com',\n",
       "  'soumendra',\n",
       "  'learn',\n",
       "  'spark',\n",
       "  'python',\n",
       "  'blob',\n",
       "  'master',\n",
       "  'setting',\n",
       "  'up',\n",
       "  'md',\n",
       "  ''],\n",
       " ['2',\n",
       "  'go',\n",
       "  'through',\n",
       "  'the',\n",
       "  'pre',\n",
       "  'class',\n",
       "  'reading',\n",
       "  'list',\n",
       "  'https',\n",
       "  'github',\n",
       "  'com',\n",
       "  'soumendra',\n",
       "  'learn',\n",
       "  'spark',\n",
       "  'python',\n",
       "  'blob',\n",
       "  'master',\n",
       "  'pre',\n",
       "  'course',\n",
       "  'reading',\n",
       "  'md',\n",
       "  ''],\n",
       " ['', 'setting', 'up', 'aws', 'instance'],\n",
       " ['when',\n",
       "  'you',\n",
       "  'are',\n",
       "  'logging',\n",
       "  'into',\n",
       "  'a',\n",
       "  'new',\n",
       "  'ec2',\n",
       "  'instance',\n",
       "  'for',\n",
       "  'the',\n",
       "  'first',\n",
       "  'time',\n",
       "  'execute',\n",
       "  'the',\n",
       "  'following',\n",
       "  ''],\n",
       " ['sudo', 'apt', 'get', 'update', 'y'],\n",
       " ['sudo', 'apt', 'get', 'upgrade', 'y'],\n",
       " ['sudo',\n",
       "  'apt',\n",
       "  'get',\n",
       "  'install',\n",
       "  'y',\n",
       "  'python',\n",
       "  'dev',\n",
       "  'software',\n",
       "  'properties',\n",
       "  'common',\n",
       "  'curl',\n",
       "  'default',\n",
       "  'jre'],\n",
       " ['sudo',\n",
       "  'apt',\n",
       "  'get',\n",
       "  'install',\n",
       "  'y',\n",
       "  'default',\n",
       "  'jdk',\n",
       "  'python',\n",
       "  'software',\n",
       "  'properties',\n",
       "  'byobu',\n",
       "  'vim'],\n",
       " ['sudo', 'apt', 'get', 'install', 'git', 'git', 'core'],\n",
       " ['git', 'config', 'global', 'user', 'email', 'you', 'example', 'com', ''],\n",
       " ['git', 'config', 'global', 'user', 'name', 'your', 'name', ''],\n",
       " ['',\n",
       "  'set',\n",
       "  'up',\n",
       "  'anaconda',\n",
       "  'https',\n",
       "  'github',\n",
       "  'com',\n",
       "  'soumendra',\n",
       "  'python',\n",
       "  'machinelearning',\n",
       "  'setup'],\n",
       " ['', 'clone', 'and', 'install', 'from', 'spark', 'yml'],\n",
       " ['jupyter', 'notebook', 'generate', 'config'],\n",
       " ['', 'c', 'get_config', ''],\n",
       " ['',\n",
       "  'c',\n",
       "  'notebookapp',\n",
       "  'certfile',\n",
       "  'u',\n",
       "  'home',\n",
       "  'ubuntu',\n",
       "  'certs',\n",
       "  'mycert',\n",
       "  'pem',\n",
       "  ''],\n",
       " ['', 'c', 'notebookapp', 'ip', ''],\n",
       " ['', 'c', 'notebookapp', 'open_browser', 'false'],\n",
       " ['', 'c', 'notebookapp', 'port', '8888']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = filein.map(lambda line: re.split('\\W+', line.lower().strip()))\n",
    "words = words.filter(lambda x: len(x) > 3)\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words with more than 3 characters in file: 28\n"
     ]
    }
   ],
   "source": [
    "words = filein.map(lambda w: (w, 1))\n",
    "words = words.reduceByKey(add)\n",
    "print('number of words with more than 3 characters in file: %s' % words.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 5: Workflow Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Spark Application Template - execute with spark-submit\n",
    "\n",
    "## Imports\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "## Module Constants\n",
    "APP_NAME = \"Name of Application\"  #helps in debugging\n",
    "\n",
    "## Closure Functions\n",
    "\n",
    "## Main functionality\n",
    "\n",
    "def main(sc):\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    conf = conf.setMaster(\"local[*]\")\n",
    "    sc   = SparkContext(conf=conf)\n",
    "\n",
    "    # Execute Main functionality\n",
    "    main(sc)\n",
    "\n",
    "# To close or exit the program use sc.stop() or sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from add import *\n",
    "\n",
    "add(5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 6: Sample Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# provide path to your spark directory directly\n",
    "findspark.init(\"/Users/soumendra/spark2/\")\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "## Imports\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from operator import add, itemgetter\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "## Module Constants\n",
    "APP_NAME = \"Flight Delay Analysis\"\n",
    "DATE_FMT = \"%Y-%m-%d\"\n",
    "TIME_FMT = \"%H%M\"\n",
    "\n",
    "fields   = ('date', 'airline', 'flightnum', 'origin', 'dest', 'dep',\n",
    "            'dep_delay', 'arv', 'arv_delay', 'airtime', 'distance')\n",
    "Flight   = namedtuple('Flight', fields)\n",
    "\n",
    "## Closure Functions\n",
    "def parse(row):\n",
    "    \"\"\"\n",
    "    Parses a row and returns a named tuple.\n",
    "    \"\"\"\n",
    "\n",
    "    row[0]  = datetime.strptime(row[0], DATE_FMT).date()\n",
    "    row[5]  = datetime.strptime(row[5], TIME_FMT).time()\n",
    "    row[6]  = float(row[6])\n",
    "    row[7]  = datetime.strptime(row[7], TIME_FMT).time()\n",
    "    row[8]  = float(row[8])\n",
    "    row[9]  = float(row[9])\n",
    "    row[10] = float(row[10])\n",
    "    return Flight(*row[:11])\n",
    "\n",
    "def split(line):\n",
    "    \"\"\"\n",
    "    Operator function for splitting a line with csv module\n",
    "    \"\"\"\n",
    "    reader = csv.reader(StringIO(line))\n",
    "    return reader.next()\n",
    "\n",
    "def plot(delays):\n",
    "    \"\"\"\n",
    "    Show a bar chart of the total delay per airline\n",
    "    \"\"\"\n",
    "    airlines = [d[0] for d in delays]\n",
    "    minutes  = [d[1] for d in delays]\n",
    "    index    = list(xrange(len(airlines)))\n",
    "\n",
    "    fig, axe = plt.subplots()\n",
    "    bars = axe.barh(index, minutes)\n",
    "\n",
    "    # Add the total minutes to the right\n",
    "    for idx, air, min in zip(index, airlines, minutes):\n",
    "        if min > 0:\n",
    "            bars[idx].set_color('#d9230f')\n",
    "            axe.annotate(\" %0.0f min\" % min, xy=(min+1, idx+0.5), va='center')\n",
    "        else:\n",
    "            bars[idx].set_color('#469408')\n",
    "            axe.annotate(\" %0.0f min\" % min, xy=(10, idx+0.5), va='center')\n",
    "\n",
    "    # Set the ticks\n",
    "    ticks = plt.yticks([idx+ 0.5 for idx in index], airlines)\n",
    "    xt = plt.xticks()[0]\n",
    "    plt.xticks(xt, [' '] * len(xt))\n",
    "\n",
    "    # minimize chart junk\n",
    "    plt.grid(axis = 'x', color ='white', linestyle='-')\n",
    "\n",
    "    plt.title('Total Minutes Delayed per Airline')\n",
    "    plt.show()\n",
    "\n",
    "## Main functionality\n",
    "def main(sc):\n",
    "\n",
    "    # Load the airlines lookup dictionary\n",
    "    airlines = dict(sc.textFile(\"ontime/airlines.csv\").map(split).collect())\n",
    "\n",
    "    # Broadcast the lookup dictionary to the cluster\n",
    "    airline_lookup = sc.broadcast(airlines)\n",
    "\n",
    "    # Read the CSV Data into an RDD\n",
    "    flights = sc.textFile(\"ontime/flights.csv\").map(split).map(parse)\n",
    "\n",
    "    # Map the total delay to the airline (joined using the broadcast value)\n",
    "    delays  = flights.map(lambda f: (airline_lookup.value[f.airline],\n",
    "                                     add(f.dep_delay, f.arv_delay)))\n",
    "\n",
    "    # Reduce the total delay for the month to the airline\n",
    "    delays  = delays.reduceByKey(add).collect()\n",
    "    delays  = sorted(delays, key=itemgetter(1))\n",
    "\n",
    "    # Provide output from the driver\n",
    "    for d in delays:\n",
    "        print(\"%0.0f minutes delayed\\t%s\" % (d[1], d[0]))\n",
    "\n",
    "    # Show a bar chart of the delays\n",
    "    plot(delays)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    conf = SparkConf().setMaster(\"local[*]\")\n",
    "    conf = conf.setAppName(APP_NAME)\n",
    "    sc   = SparkContext(conf=conf)\n",
    "    # Uncomment the lines above when running the application with \"submit\" (spark-submit app.py)\n",
    "    # Comment the lines above out when running in IPython Notebook\n",
    "\n",
    "    # Execute Main functionality\n",
    "    main(sc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
